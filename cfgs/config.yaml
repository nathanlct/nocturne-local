defaults:
  - algorithm: ppo
  - override hydra/launcher: submitit_local

seed: 0
device: 'cuda:0'
debug: False
experiment: intersection
env: my_custom_multi_env_v1 # name of the env, harcoded for now

# WANDB things
wandb: True
wandb_project: nocturne4
wandb_id: null
wandb_group: ${experiment}

# one of the agents will be randomly tagged as the 
# agent that we control, the rest of the agents will
# replay trajectories
single_agent_mode: False
# all goals are achievable within 90 steps
episode_length: 90
# how many files of the total dataset to use. -1 indicates to use all of them
num_files: -1
scenario_path: '/checkpoint/eugenevinitsky/waymo_open/motion_v1p1/uncompressed/scenario/formatted_json_v2_no_tl_train'
dt: 0.1
sims_per_step: 10
img_as_state: False
discretize_actions: True
accel_discretization: 9
accel_lower_bound: -3
accel_upper_bound: 2
steering_lower_bound: -0.7 # corresponds to about 40 degrees of max steering angle
steering_upper_bound: 0.7 # corresponds to about 40 degrees of max steering angle
steering_discretization: 9
max_num_vehicles: 15 # we want to upper bound how many agents there can be in the scene
# TODO(eugenevinitsky) actually implement this
randomize_goals: True
rew_cfg:
  shared_reward: False # agents get the collective reward instead of individual rewards
  goal_tolerance: 0.5
  reward_scaling: 10.0 # rescale all the rewards by this value. This can help w/ some learning algorithms
  collision_penalty: 0
  shaped_goal_distance: True
  goal_distance_penalty: False # if shaped_goal_distance is true, then when this is True the goal distance is a penalty for being far from 
                               # goal instead of a reward for being close
  goal_achieved_bonus: ${episode_length}
subscriber:
  view_angle: 3.14
  # the distance which the cone extends before agents are not visible
  # TODO(eugenevinitsky) pick the right number
  view_dist: 120
  use_ego_state: True
  use_observations: True
  # if true, we return an observation for agents that have exited the system
  # as well as returning an observation for the extra agents if the number of
  # agents in the system is less than max_num_vehicles
  keep_inactive_agents: False

hydra:
  run:
    dir: /checkpoint/eugenevinitsky/nocturne/test/${now:%Y.%m.%d}/${experiment}/${now:%H.%M.%S}/${hydra.job.override_dirname}
  sweep:
    dir: /checkpoint/eugenevinitsky/nocturne/sweep/${now:%Y.%m.%d}/${experiment}/${now:%H.%M.%S}
    subdir: ${hydra.job.num}
  launcher:
    timeout_min: 2880
    cpus_per_task: 80
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 160
    nodes: 1
    submitit_folder: /checkpoint/eugenevinitsky/nocturne/sweep/${now:%Y.%m.%d}/${now:%H%M}_${experiment}/.slurm
